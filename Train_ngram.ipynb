{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "full = []\n",
    "for dirname, _, filenames in os.walk('Train_Full'):\n",
    "    for filename in filenames:\n",
    "        with open(os.path.join(\"\", os.path.join(dirname, filename)), 'r', encoding='UTF-16') as f:\n",
    "            \n",
    "            full.append(f.read())\n",
    "        \n",
    "            # raw_text = raw_text.replace(\"\\n\", \" \")\n",
    "            # raw_text = raw_text.lower()\n",
    "            # # raw_text = sent_tokenize(raw_text)\n",
    "            # for text in raw_text:\n",
    "            #     full.append(text)\n",
    "            \n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42744/42744 [41:56<00:00, 16.99it/s]  \n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re\n",
    "def get_token(list_str):\n",
    "    temp=[]\n",
    "    for raw_text in tqdm(list_str):\n",
    "        raw_text = raw_text.lower()\n",
    "        raw_text = raw_text.replace(\"\\n\", \"\")\n",
    "        raw_text = raw_text.replace(\"- \", \"\")\n",
    "        raw_text = sent_tokenize(raw_text)\n",
    "        sentence_list = [sentence.translate(str.maketrans('', '', string.punctuation+\"“\"+\"”\")) for sentence in raw_text]\n",
    "        for item in sentence_list:\n",
    "            \n",
    "            tokens = word_tokenize(item)\n",
    "            temp.append(tokens)\n",
    "        \n",
    "    return temp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sents = [re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', data_str) for data_str in data_str]\n",
    "\n",
    "data = get_token(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['song', 'thực tế', 'việc', 'áp dụng', 'chủ trương', 'giảm', 'tải', 'với', 'các', 'trường', 'trên', 'địa bàn', 'thủ đô', 'hà nội', 'hiện nay', 'vẫn', 'thể hiện', 'sự', 'lúng túng', 'từ', 'chính', 'cán bộ', 'quản lý', 'đến', 'giáo viên', 'đứng lớp', '…', 'trong', 'buổi', 'tổng kết', '3', 'năm', 'thay', 'sách', 'ở', 'bậc', 'tiểu học', 'nhiều', 'ý kiến', 'cho', 'rằng', 'nhiều', 'bài', 'trong', 'chương trình', 'nặng', 'so', 'với', 'khả năng', 'tiếp thu', 'của', 'học sinh', 'hs']\n"
     ]
    }
   ],
   "source": [
    "print(data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "train_data, padded_sents = padded_everygram_pipeline(3, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421355\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import MLE, Laplace, KneserNeyInterpolated, WittenBellInterpolated\n",
    "import pickle\n",
    "\n",
    "n=3\n",
    "\n",
    "vi_model = KneserNeyInterpolated(n)\n",
    "\n",
    "vi_model.fit(train_data, padded_sents)\n",
    "print(len(vi_model.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join('kneserney_1st_ngram_model.pkl'), 'wb') as fout:\n",
    "    pickle.dump(vi_model, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tổ chức vô địch mùa đông hà nội thời tiết ấm áp sẽ còn tăng đến'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, pre_words=[]):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = pre_words\n",
    "    for i in range(num_words):\n",
    "        token = model.generate(1, text_seed=content[-2:])\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)\n",
    "\n",
    "generate_sent(vi_model, 10, [\"tổ\", \"chức\"])\n",
    "#đất nước việt nam các phương tiện như ở vn sử dụng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
